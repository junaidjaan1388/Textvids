name: WanT2V Video Generation

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt for video generation'
        required: true
        default: 'A beautiful sunset over mountains'
      width:
        description: 'Video width'
        required: true
        default: 1280
      height:
        description: 'Video height'
        required: true
        default: 720
      frames:
        description: 'Number of frames'
        required: true
        default: 81
      seed:
        description: 'Random seed (-1 for random)'
        required: true
        default: -1

jobs:
  generate-video:
    runs-on: ubuntu-latest
    # Remove container section to use host runner

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-pip python3-venv git

    - name: Install Python dependencies
      run: |
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
        pip install tqdm transformers accelerate diffusers
        pip install opencv-python pillow

    - name: Create test script
      run: |
        cat > test_generation.py << 'EOF'
        import torch
        import os
        import sys
        import tempfile
        
        # Add your module to path
        sys.path.append('.')
        
        # Mock imports for testing - replace with your actual imports
        try:
            from your_module import WanT2V
            from your_module.config import get_config
        except ImportError:
            print("Using mock implementation for testing")
            # Mock implementation for testing
            class MockConfig:
                def __init__(self):
                    self.num_train_timesteps = 1000
                    self.param_dtype = torch.float32
                    self.text_len = 120
                    self.t5_dtype = torch.float32
                    self.t5_checkpoint = 'mock'
                    self.t5_tokenizer = 'mock'
                    self.vae_stride = (1, 4, 4)
                    self.patch_size = (2, 2, 2)
                    self.vae_checkpoint = 'mock'
                    self.sample_neg_prompt = ''
            
            class MockWanT2V:
                def __init__(self, config, checkpoint_dir, **kwargs):
                    self.config = config
                    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    print('WanT2V initialization successful')
                
                def generate(self, input_prompt, size, frame_num, **kwargs):
                    print(f"Mock generation: {input_prompt}, size: {size}, frames: {frame_num}")
                    # Return mock tensor
                    return torch.randn(3, frame_num, size[1], size[0])
            
            WanT2V = MockWanT2V
            get_config = MockConfig

        def main():
            # Initialize configuration
            config = get_config()
            
            # Set up checkpoint directory
            checkpoint_dir = "checkpoints"
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # Get workflow inputs from environment
            prompt = os.getenv('INPUT_PROMPT', 'A beautiful sunset over mountains')
            width = int(os.getenv('INPUT_WIDTH', '1280'))
            height = int(os.getenv('INPUT_HEIGHT', '720'))
            frames = int(os.getenv('INPUT_FRAMES', '81'))
            seed = int(os.getenv('INPUT_SEED', '-1'))
            
            print(f"Generating video with prompt: {prompt}")
            print(f"Resolution: {width}x{height}, Frames: {frames}, Seed: {seed}")
            
            try:
                # Initialize model
                model = WanT2V(
                    config=config,
                    checkpoint_dir=checkpoint_dir,
                    device_id=0,
                    rank=0,
                    t5_fsdp=False,
                    dit_fsdp=False,
                    use_usp=False,
                    t5_cpu=False
                )
                
                # Generate video
                with torch.no_grad():
                    video = model.generate(
                        input_prompt=prompt,
                        size=(width, height),
                        frame_num=frames,
                        shift=5.0,
                        sample_solver='unipc',
                        sampling_steps=20,  # Reduced for faster testing
                        guide_scale=5.0,
                        n_prompt="",
                        seed=seed,
                        offload_model=True
                    )
                
                if video is not None:
                    # Create output directory
                    output_dir = "output_frames"
                    os.makedirs(output_dir, exist_ok=True)
                    
                    # Save video metadata
                    with open(f"{output_dir}/metadata.txt", "w") as f:
                        f.write(f"Prompt: {prompt}\n")
                        f.write(f"Resolution: {width}x{height}\n")
                        f.write(f"Frames: {frames}\n")
                        f.write(f"Seed: {seed}\n")
                        f.write(f"Video shape: {video.shape}\n")
                    
                    print("Video generation completed successfully!")
                    print(f"Output shape: {video.shape}")
                    
                    # Create sample output files
                    for i in range(min(5, frames)):  # Save first 5 frames as sample
                        with open(f"{output_dir}/frame_{i:04d}.txt", "w") as f:
                            f.write(f"Frame {i} - Mock data\n")
                    
                else:
                    print("Video generation failed - no output")
                    # Create error file
                    with open("error_log.txt", "w") as f:
                        f.write("Video generation failed")
                    
            except Exception as e:
                print(f"Error during video generation: {e}")
                # Create error file
                with open("error_log.txt", "w") as f:
                    f.write(f"Error: {e}")
                raise

        if __name__ == "__main__":
            main()
        EOF

    - name: Run video generation
      env:
        INPUT_PROMPT: ${{ github.event.inputs.prompt }}
        INPUT_WIDTH: ${{ github.event.inputs.width }}
        INPUT_HEIGHT: ${{ github.event.inputs.height }}
        INPUT_FRAMES: ${{ github.event.inputs.frames }}
        INPUT_SEED: ${{ github.event.inputs.seed }}
      run: |
        python test_generation.py

    - name: Upload generated frames
      uses: actions/upload-artifact@v4
      with:
        name: generated-frames
        path: |
          output_frames/
          *.txt
        retention-days: 30

    - name: Upload logs
      uses: actions/upload-artifact@v4
      with:
        name: generation-logs
        path: |
          *.log
          *.txt
        retention-days: 7
