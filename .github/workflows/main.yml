name: WanT2V Video Generation

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt for video generation'
        required: true
        default: 'A beautiful sunset over mountains'
      width:
        description: 'Video width'
        required: true
        default: 1280
      height:
        description: 'Video height'
        required: true
        default: 720
      frames:
        description: 'Number of frames'
        required: true
        default: 81
      seed:
        description: 'Random seed (-1 for random)'
        required: true
        default: -1

jobs:
  generate-video:
    runs-on: ubuntu-latest
    container:
      image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
      options: --gpus all

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install system dependencies
      run: |
        apt-get update
        apt-get install -y git libgl1-mesa-glx libglib2.0-0

    - name: Install Python dependencies
      run: |
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
        pip install tqdm transformers accelerate
        pip install git+https://github.com/huggingface/diffusers.git

    - name: Create model directory structure
      run: |
        mkdir -p checkpoints/models
        # Note: You'll need to add your actual model checkpoints here
        # This is just the directory structure
        echo "Model directory created - add your checkpoints manually"

    - name: Create test script
      run: |
        cat > test_generation.py << 'EOF'
        import torch
        import os
        from your_module import WanT2V  # Adjust import path as needed
        from your_module.config import get_config  # Adjust import path as needed

        def main():
            # Initialize configuration
            config = get_config()  # You'll need to implement this based on your config structure
            
            # Set up checkpoint directory
            checkpoint_dir = "checkpoints"
            
            # Get workflow inputs
            prompt = "${{ github.event.inputs.prompt }}"
            width = int("${{ github.event.inputs.width }}")
            height = int("${{ github.event.inputs.height }}")
            frames = int("${{ github.event.inputs.frames }}")
            seed = int("${{ github.event.inputs.seed }}")
            
            print(f"Generating video with prompt: {prompt}")
            print(f"Resolution: {width}x{height}, Frames: {frames}, Seed: {seed}")
            
            try:
                # Initialize model
                model = WanT2V(
                    config=config,
                    checkpoint_dir=checkpoint_dir,
                    device_id=0,
                    rank=0,
                    t5_fsdp=False,
                    dit_fsdp=False,
                    use_usp=False,
                    t5_cpu=False
                )
                
                # Generate video
                with torch.no_grad():
                    video = model.generate(
                        input_prompt=prompt,
                        size=(width, height),
                        frame_num=frames,
                        shift=5.0,
                        sample_solver='unipc',
                        sampling_steps=20,  # Reduced for faster testing
                        guide_scale=5.0,
                        n_prompt="",
                        seed=seed,
                        offload_model=True
                    )
                
                if video is not None:
                    # Save video as individual frames
                    os.makedirs("output_frames", exist_ok=True)
                    for i in range(video.shape[1]):
                        frame = video[:, i]  # Get individual frame
                        # You'll need to implement frame saving logic here
                        # torchvision.utils.save_frame or similar
                    
                    print("Video generation completed successfully!")
                    print(f"Output shape: {video.shape}")
                else:
                    print("Video generation failed - no output")
                    
            except Exception as e:
                print(f"Error during video generation: {e}")
                raise

        if __name__ == "__main__":
            main()
        EOF

    - name: Run video generation
      run: |
        python test_generation.py

    - name: Upload generated frames
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: generated-frames
        path: output_frames/
        retention-days: 1

    - name: Upload logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: generation-logs
        path: |
          *.log
        retention-days: 7

  setup-notification:
    runs-on: ubuntu-latest
    needs: generate-video
    steps:
    - name: Notify completion
      run: |
        echo "Video generation job completed with status: ${{ needs.generate-video.result }}"
