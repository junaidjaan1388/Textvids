name: WanT2V Video Generation

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt for video generation'
        required: true
        default: 'A beautiful sunset over mountains'
      width:
        description: 'Video width'
        required: true
        default: 1280
      height:
        description: 'Video height'
        required: true
        default: 720
      frames:
        description: 'Number of frames'
        required: true
        default: 81
      seed:
        description: 'Random seed (-1 for random)'
        required: true
        default: -1

jobs:
  generate-video:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-pip python3-venv git ffmpeg zip

    - name: Install Python dependencies
      run: |
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
        pip install tqdm transformers accelerate diffusers
        pip install opencv-python pillow imageio imageio-ffmpeg

    - name: Create test script
      run: |
        cat > test_generation.py << 'EOF'
        import torch
        import os
        import sys
        import numpy as np
        import zipfile
        from PIL import Image
        import imageio
        
        # Add your module to path
        sys.path.append('.')
        
        # Mock imports for testing - replace with your actual imports
        try:
            from your_module import WanT2V
            from your_module.config import get_config
        except ImportError:
            print("Using mock implementation for testing")
            # Mock implementation for testing
            class MockConfig:
                def __init__(self):
                    self.num_train_timesteps = 1000
                    self.param_dtype = torch.float32
                    self.text_len = 120
                    self.t5_dtype = torch.float32
                    self.t5_checkpoint = 'mock'
                    self.t5_tokenizer = 'mock'
                    self.vae_stride = (1, 4, 4)
                    self.patch_size = (2, 2, 2)
                    self.vae_checkpoint = 'mock'
                    self.sample_neg_prompt = ''
            
            class MockWanT2V:
                def __init__(self, config, checkpoint_dir, **kwargs):
                    self.config = config
                    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    print('WanT2V initialization successful')
                
                def generate(self, input_prompt, size, frame_num, **kwargs):
                    print(f"Mock generation: {input_prompt}, size: {size}, frames: {frame_num}")
                    # Create mock video tensor (C, N, H, W)
                    return torch.randn(3, frame_num, size[1], size[0])
            
            WanT2V = MockWanT2V
            get_config = MockConfig

        def save_video_as_mp4(video_tensor, output_path, fps=24):
            """Save video tensor as MP4 file"""
            # Convert tensor to numpy and scale to 0-255
            video_np = video_tensor.cpu().numpy()
            video_np = (video_np - video_np.min()) / (video_np.max() - video_np.min()) * 255
            video_np = video_np.astype(np.uint8)
            
            # Rearrange dimensions to (N, H, W, C)
            video_np = video_np.transpose(1, 2, 3, 0)
            
            # Save as MP4
            with imageio.get_writer(output_path, fps=fps) as writer:
                for frame in video_np:
                    writer.append_data(frame)
            
            print(f"Video saved as: {output_path}")

        def save_frames_as_images(video_tensor, output_dir):
            """Save individual frames as PNG images"""
            os.makedirs(output_dir, exist_ok=True)
            video_np = video_tensor.cpu().numpy()
            video_np = (video_np - video_np.min()) / (video_np.max() - video_np.min()) * 255
            video_np = video_np.astype(np.uint8)
            
            for i in range(video_np.shape[1]):
                frame = video_np[:, i]  # Get frame (C, H, W)
                frame = frame.transpose(1, 2, 0)  # Convert to (H, W, C)
                img = Image.fromarray(frame)
                img.save(os.path.join(output_dir, f"frame_{i:04d}.png"))

        def create_zip_archive(source_dir, output_zip):
            """Create ZIP archive of directory"""
            with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(source_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, source_dir)
                        zipf.write(file_path, arcname)
            print(f"Created ZIP archive: {output_zip}")

        def main():
            # Initialize configuration
            config = get_config()
            
            # Set up checkpoint directory
            checkpoint_dir = "checkpoints"
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # Get workflow inputs from environment
            prompt = os.getenv('INPUT_PROMPT', 'A beautiful sunset over mountains')
            width = int(os.getenv('INPUT_WIDTH', '1280'))
            height = int(os.getenv('INPUT_HEIGHT', '720'))
            frames = int(os.getenv('INPUT_FRAMES', '81'))
            seed = int(os.getenv('INPUT_SEED', '-1'))
            
            print(f"Generating video with prompt: {prompt}")
            print(f"Resolution: {width}x{height}, Frames: {frames}, Seed: {seed}")
            
            try:
                # Initialize model
                model = WanT2V(
                    config=config,
                    checkpoint_dir=checkpoint_dir,
                    device_id=0,
                    rank=0,
                    t5_fsdp=False,
                    dit_fsdp=False,
                    use_usp=False,
                    t5_cpu=False
                )
                
                # Generate video
                with torch.no_grad():
                    video = model.generate(
                        input_prompt=prompt,
                        size=(width, height),
                        frame_num=frames,
                        shift=5.0,
                        sample_solver='unipc',
                        sampling_steps=20,  # Reduced for faster testing
                        guide_scale=5.0,
                        n_prompt="",
                        seed=seed,
                        offload_model=True
                    )
                
                if video is not None:
                    # Create output directories
                    output_dir = "generated_video"
                    frames_dir = os.path.join(output_dir, "frames")
                    os.makedirs(output_dir, exist_ok=True)
                    os.makedirs(frames_dir, exist_ok=True)
                    
                    # Save video as MP4
                    mp4_path = os.path.join(output_dir, "generated_video.mp4")
                    save_video_as_mp4(video, mp4_path, fps=24)
                    
                    # Save individual frames
                    save_frames_as_images(video, frames_dir)
                    
                    # Create metadata
                    metadata_path = os.path.join(output_dir, "metadata.txt")
                    with open(metadata_path, "w") as f:
                        f.write(f"Prompt: {prompt}\n")
                        f.write(f"Resolution: {width}x{height}\n")
                        f.write(f"Frames: {frames}\n")
                        f.write(f"Seed: {seed}\n")
                        f.write(f"Video shape: {video.shape}\n")
                        f.write(f"FPS: 24\n")
                        f.write(f"Duration: {frames/24:.2f} seconds\n")
                    
                    # Create ZIP archive
                    zip_path = "generated_video_output.zip"
                    create_zip_archive(output_dir, zip_path)
                    
                    print("Video generation completed successfully!")
                    print(f"Output shape: {video.shape}")
                    print(f"ZIP file created: {zip_path}")
                    
                else:
                    print("Video generation failed - no output")
                    # Create error file
                    with open("error_log.txt", "w") as f:
                        f.write("Video generation failed")
                    
            except Exception as e:
                print(f"Error during video generation: {e}")
                import traceback
                traceback.print_exc()
                # Create error file
                with open("error_log.txt", "w") as f:
                    f.write(f"Error: {e}\n")
                    f.write(traceback.format_exc())
                raise

        if __name__ == "__main__":
            main()
        EOF

    - name: Run video generation
      env:
        INPUT_PROMPT: ${{ github.event.inputs.prompt }}
        INPUT_WIDTH: ${{ github.event.inputs.width }}
        INPUT_HEIGHT: ${{ github.event.inputs.height }}
        INPUT_FRAMES: ${{ github.event.inputs.frames }}
        INPUT_SEED: ${{ github.event.inputs.seed }}
      run: |
        python test_generation.py

    - name: List generated files
      run: |
        echo "Generated files:"
        find . -name "*.zip" -o -name "*.mp4" -o -name "*.txt" -o -name "*.log" | head -20

    - name: Upload video ZIP file
      uses: actions/upload-artifact@v4
      with:
        name: generated-video-zip
        path: |
          generated_video_output.zip
          generated_video/
        retention-days: 30

    - name: Upload individual files as backup
      uses: actions/upload-artifact@v4
      with:
        name: video-files-backup
        path: |
          generated_video/generated_video.mp4
          generated_video/metadata.txt
        retention-days: 7

    - name: Upload logs
      uses: actions/upload-artifact@v4
      with:
        name: generation-logs
        path: |
          *.log
          *.txt
        retention-days: 7
