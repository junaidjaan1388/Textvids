name: WanT2V Video Generator

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt for video generation'
        required: true
        default: 'A beautiful sunset over mountains'
      width:
        description: 'Video width'
        required: true
        default: 1280
      height:
        description: 'Video height'
        required: true
        default: 720
      frames:
        description: 'Number of frames'
        required: true
        default: 81
      seed:
        description: 'Random seed (-1 for random)'
        required: true
        default: -1
      sampling_steps:
        description: 'Number of sampling steps'
        required: true
        default: 20
      guide_scale:
        description: 'Guidance scale'
        required: true
        default: 5.0

jobs:
  generate-video:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-pip python3-venv git ffmpeg zip

    - name: Install Python dependencies
      run: |
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
        pip install tqdm transformers accelerate diffusers
        pip install opencv-python pillow imageio imageio-ffmpeg matplotlib

    - name: Create video generation script
      run: |
        cat > video_generator.py << 'EOF'
        import torch
        import os
        import sys
        import numpy as np
        import zipfile
        from PIL import Image
        import imageio
        import json
        from datetime import datetime
        
        # Add your module to path
        sys.path.append('.')
        
        # Mock imports for testing - replace with your actual imports
        try:
            from your_module import WanT2V
            from your_module.config import get_config
            REAL_MODEL = True
        except ImportError:
            print("Using mock implementation for testing")
            REAL_MODEL = False
            # Mock implementation for testing
            class MockConfig:
                def __init__(self):
                    self.num_train_timesteps = 1000
                    self.param_dtype = torch.float32
                    self.text_len = 120
                    self.t5_dtype = torch.float32
                    self.t5_checkpoint = 'mock'
                    self.t5_tokenizer = 'mock'
                    self.vae_stride = (1, 4, 4)
                    self.patch_size = (2, 2, 2)
                    self.vae_checkpoint = 'mock'
                    self.sample_neg_prompt = ''
            
            class MockWanT2V:
                def __init__(self, config, checkpoint_dir, **kwargs):
                    self.config = config
                    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    print('WanT2V initialization successful')
                
                def generate(self, input_prompt, size, frame_num, **kwargs):
                    print(f"Generating video: {input_prompt}")
                    print(f"Size: {size}, Frames: {frame_num}")
                    print(f"Parameters: {kwargs}")
                    
                    # Create more realistic mock video with some temporal coherence
                    frames = []
                    base_noise = torch.randn(3, size[1], size[0])
                    
                    for i in range(frame_num):
                        # Add some temporal variation
                        noise = base_noise + 0.1 * torch.randn(3, size[1], size[0])
                        frame = torch.sigmoid(noise) * 255
                        frames.append(frame)
                    
                    video = torch.stack(frames, dim=1)  # (C, N, H, W)
                    print(f"Generated video shape: {video.shape}")
                    return video
            
            WanT2V = MockWanT2V
            get_config = MockConfig

        class VideoGenerator:
            def __init__(self):
                self.config = get_config()
                self.checkpoint_dir = "checkpoints"
                os.makedirs(self.checkpoint_dir, exist_ok=True)
                
            def tensor_to_uint8(self, tensor):
                """Convert tensor to uint8 numpy array"""
                tensor = tensor.cpu().float()
                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min()) * 255
                return tensor.numpy().astype(np.uint8)
            
            def save_video_mp4(self, video_tensor, output_path, fps=24):
                """Save video tensor as MP4 file"""
                video_np = self.tensor_to_uint8(video_tensor)
                
                # Rearrange dimensions to (N, H, W, C)
                if video_np.shape[0] == 3:  # (C, N, H, W) -> (N, H, W, C)
                    video_np = video_np.transpose(1, 2, 3, 0)
                
                # Save as MP4
                with imageio.get_writer(output_path, fps=fps, quality=8) as writer:
                    for frame in video_np:
                        writer.append_data(frame)
                
                print(f"Video saved as: {output_path}")
                return output_path
            
            def save_frames_as_images(self, video_tensor, output_dir, max_frames=50):
                """Save individual frames as PNG images"""
                os.makedirs(output_dir, exist_ok=True)
                video_np = self.tensor_to_uint8(video_tensor)
                
                if video_np.shape[0] == 3:  # (C, N, H, W)
                    video_np = video_np.transpose(1, 2, 3, 0)  # (N, H, W, C)
                
                # Save frames (limit to max_frames to avoid too many files)
                frames_to_save = min(video_np.shape[0], max_frames)
                step = max(1, video_np.shape[0] // frames_to_save)
                
                for i in range(0, video_np.shape[0], step):
                    if i >= frames_to_save * step:
                        break
                    frame = video_np[i]
                    img = Image.fromarray(frame)
                    img.save(os.path.join(output_dir, f"frame_{i:04d}.png"))
                
                print(f"Saved {frames_to_save} frames to {output_dir}")
            
            def create_preview_gif(self, video_tensor, output_path, fps=10, duration=5):
                """Create preview GIF from video"""
                video_np = self.tensor_to_uint8(video_tensor)
                
                if video_np.shape[0] == 3:  # (C, N, H, W)
                    video_np = video_np.transpose(1, 2, 3, 0)  # (N, H, W, C)
                
                # Resize for smaller GIF
                height, width = video_np.shape[1:3]
                new_height = min(256, height)
                new_width = int(width * (new_height / height))
                
                # Select frames for GIF
                total_frames = min(video_np.shape[0], int(fps * duration))
                frame_indices = np.linspace(0, video_np.shape[0]-1, total_frames, dtype=int)
                
                frames = []
                for idx in frame_indices:
                    frame = Image.fromarray(video_np[idx])
                    frame = frame.resize((new_width, new_height), Image.LANCZOS)
                    frames.append(frame)
                
                # Save as GIF
                frames[0].save(
                    output_path,
                    save_all=True,
                    append_images=frames[1:],
                    duration=1000//fps,
                    loop=0
                )
                print(f"Preview GIF saved: {output_path}")
            
            def create_zip_archive(self, source_dirs, output_zip):
                """Create ZIP archive of multiple directories"""
                with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for source_dir in source_dirs:
                        if os.path.exists(source_dir):
                            for root, dirs, files in os.walk(source_dir):
                                for file in files:
                                    file_path = os.path.join(root, file)
                                    arcname = os.path.relpath(file_path, os.path.dirname(source_dir))
                                    zipf.write(file_path, arcname)
                print(f"Created ZIP archive: {output_zip}")
                return output_zip
            
            def generate_video(self, params):
                """Main video generation function"""
                print("ğŸš€ Starting WanT2V Video Generation")
                print(f"Parameters: {params}")
                
                # Initialize model
                model = WanT2V(
                    config=self.config,
                    checkpoint_dir=self.checkpoint_dir,
                    device_id=0,
                    rank=0,
                    t5_fsdp=False,
                    dit_fsdp=False,
                    use_usp=False,
                    t5_cpu=False
                )
                
                # Generate video
                with torch.no_grad():
                    video = model.generate(
                        input_prompt=params['prompt'],
                        size=(params['width'], params['height']),
                        frame_num=params['frames'],
                        shift=5.0,
                        sample_solver='unipc',
                        sampling_steps=params['sampling_steps'],
                        guide_scale=params['guide_scale'],
                        n_prompt="",
                        seed=params['seed'],
                        offload_model=True
                    )
                
                if video is not None:
                    return self._save_outputs(video, params)
                else:
                    raise ValueError("Video generation failed - no output")
            
            def _save_outputs(self, video, params):
                """Save all output files and create metadata"""
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_dir = f"wan_output_{timestamp}"
                os.makedirs(output_dir, exist_ok=True)
                
                # Create subdirectories
                frames_dir = os.path.join(output_dir, "frames")
                preview_dir = os.path.join(output_dir, "preview")
                os.makedirs(frames_dir, exist_ok=True)
                os.makedirs(preview_dir, exist_ok=True)
                
                print(f"ğŸ“ Output directory: {output_dir}")
                
                # Save video as MP4
                mp4_path = os.path.join(output_dir, "generated_video.mp4")
                self.save_video_mp4(video, mp4_path, fps=24)
                
                # Save frames
                self.save_frames_as_images(video, frames_dir)
                
                # Create preview GIF
                gif_path = os.path.join(preview_dir, "preview.gif")
                self.create_preview_gif(video, gif_path)
                
                # Create metadata
                metadata = {
                    "prompt": params['prompt'],
                    "negative_prompt": "",
                    "resolution": f"{params['width']}x{params['height']}",
                    "frames": params['frames'],
                    "seed": params['seed'],
                    "sampling_steps": params['sampling_steps'],
                    "guide_scale": params['guide_scale'],
                    "video_shape": list(video.shape),
                    "fps": 24,
                    "duration": f"{params['frames']/24:.2f}s",
                    "model": "WanT2V",
                    "timestamp": timestamp,
                    "using_real_model": REAL_MODEL
                }
                
                metadata_path = os.path.join(output_dir, "metadata.json")
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
                
                # Create readme
                readme_path = os.path.join(output_dir, "README.txt")
                with open(readme_path, 'w') as f:
                    f.write("WanT2V Video Generation Output\n")
                    f.write("==============================\n\n")
                    f.write(f"Prompt: {params['prompt']}\n")
                    f.write(f"Resolution: {params['width']}x{params['height']}\n")
                    f.write(f"Frames: {params['frames']}\n")
                    f.write(f"Seed: {params['seed']}\n")
                    f.write(f"Sampling Steps: {params['sampling_steps']}\n")
                    f.write(f"Guidance Scale: {params['guide_scale']}\n")
                    f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                
                # Create ZIP archive
                zip_path = f"wan_video_{timestamp}.zip"
                self.create_zip_archive([output_dir], zip_path)
                
                print("âœ… Video generation completed successfully!")
                print(f"ğŸ“¦ ZIP file: {zip_path}")
                print(f"ğŸ¬ Video file: {mp4_path}")
                print(f"ğŸ–¼ï¸  Frames saved: {frames_dir}")
                print(f"ğŸ‘€ Preview GIF: {gif_path}")
                
                return {
                    "zip_path": zip_path,
                    "output_dir": output_dir,
                    "video_path": mp4_path,
                    "metadata": metadata
                }

        def main():
            # Get parameters from environment
            params = {
                'prompt': os.getenv('INPUT_PROMPT', 'A beautiful sunset over mountains'),
                'width': int(os.getenv('INPUT_WIDTH', '1280')),
                'height': int(os.getenv('INPUT_HEIGHT', '720')),
                'frames': int(os.getenv('INPUT_FRAMES', '81')),
                'seed': int(os.getenv('INPUT_SEED', '-1')),
                'sampling_steps': int(os.getenv('INPUT_SAMPLING_STEPS', '20')),
                'guide_scale': float(os.getenv('INPUT_GUIDE_SCALE', '5.0'))
            }
            
            print("ğŸ¬ WanT2V Video Generator")
            print("=" * 50)
            
            generator = VideoGenerator()
            
            try:
                results = generator.generate_video(params)
                
                # Write success flag
                with open("generation_success.txt", "w") as f:
                    f.write("SUCCESS\n")
                    f.write(f"ZIP: {results['zip_path']}\n")
                    f.write(f"Video: {results['video_path']}\n")
                
            except Exception as e:
                print(f"âŒ Error during video generation: {e}")
                import traceback
                traceback.print_exc()
                
                # Write error file
                with open("generation_error.txt", "w") as f:
                    f.write(f"ERROR: {e}\n")
                    f.write(traceback.format_exc())
                raise

        if __name__ == "__main__":
            main()
        EOF

    - name: Run video generation
      env:
        INPUT_PROMPT: ${{ github.event.inputs.prompt }}
        INPUT_WIDTH: ${{ github.event.inputs.width }}
        INPUT_HEIGHT: ${{ github.event.inputs.height }}
        INPUT_FRAMES: ${{ github.event.inputs.frames }}
        INPUT_SEED: ${{ github.event.inputs.seed }}
        INPUT_SAMPLING_STEPS: ${{ github.event.inputs.sampling_steps }}
        INPUT_GUIDE_SCALE: ${{ github.event.inputs.guide_scale }}
      run: |
        echo "Starting WanT2V Video Generation..."
        python video_generator.py

    - name: List generated files
      run: |
        echo "ğŸ“ Generated files:"
        find . -name "*.zip" -o -name "*.mp4" -o -name "*.gif" -o -name "wan_output_*" | head -20
        echo ""
        echo "ğŸ“Š File sizes:"
        du -sh *.zip *.mp4 wan_output_* 2>/dev/null || true

    - name: Upload video ZIP package
      uses: actions/upload-artifact@v4
      with:
        name: wan-video-package
        path: |
          wan_video_*.zip
        retention-days: 30

    - name: Upload individual video files
      uses: actions/upload-artifact@v4
      with:
        name: video-files
        path: |
          wan_output_*/generated_video.mp4
          wan_output_*/preview/preview.gif
        retention-days: 7

    - name: Upload frames and metadata
      uses: actions/upload-artifact@v4
      with:
        name: generation-assets
        path: |
          wan_output_*/metadata.json
          wan_output_*/README.txt
          wan_output_*/frames/
        retention-days: 7

    - name: Upload generation logs
      uses: actions/upload-artifact@v4
      with:
        name: generation-logs
        path: |
          generation_success.txt
          generation_error.txt
          *.log
        retention-days: 7

    - name: Display completion message
      if: success()
      run: |
        echo "ğŸ‰ WanT2V Video Generation Completed Successfully!"
        echo "ğŸ“¦ Check the 'Artifacts' section to download your generated video"
        echo "ğŸ“ The ZIP file contains:"
        echo "   - generated_video.mp4 (Full quality video)"
        echo "   - preview/preview.gif (Quick preview)"
        echo "   - frames/ (Individual frames as PNG)"
        echo "   - metadata.json (Generation parameters)"
        echo "   - README.txt (Generation info)"

    - name: Display error message
      if: failure()
      run: |
        echo "âŒ Video generation failed"
        echo "ğŸ“‹ Check the generation-logs artifact for detailed error information"
